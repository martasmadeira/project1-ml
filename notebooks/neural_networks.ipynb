{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb7f4cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 64.   56.   77.  ... 133.7 133.3 133.5]\n",
      "      Age  Height  FCVC  NCP  CH2O  FAF  TUE\n",
      "0      21    1.62     2    3     2    0    1\n",
      "1      21    1.52     3    3     3    3    0\n",
      "2      23    1.80     2    3     2    2    1\n",
      "3      27    1.80     3    3     2    2    0\n",
      "4      22    1.78     2    1     2    0    0\n",
      "...   ...     ...   ...  ...   ...  ...  ...\n",
      "2106   21    1.71     3    3     2    2    1\n",
      "2107   22    1.75     3    3     2    1    1\n",
      "2108   23    1.75     3    3     2    1    1\n",
      "2109   24    1.74     3    3     3    1    1\n",
      "2110   24    1.74     3    3     3    1    1\n",
      "\n",
      "[2111 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_theme(font_scale=1.)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"../data/ObesityDataSet_Clean.csv\")  \n",
    "\n",
    "# Quick look at dataset\n",
    "df.head()\n",
    "\n",
    "target = \"Weight\"\n",
    "feature_cols = [\"Age\", \"Height\", \"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n",
    "\n",
    "# define target and features (just to extract y)\n",
    "y = df[target].to_numpy()\n",
    "print(y)\n",
    "\n",
    "# baseline ignores X, but we still need to split the same way\n",
    "X = df[feature_cols].copy()\n",
    "print(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27cc80",
   "metadata": {},
   "source": [
    "*TWO LAYER CROSS VALIDATION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad02c922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Outer fold  λ*_i  E_test^lin  E_test^base  h*_i  E_test^ann\n",
      "0           1    10      462.83       732.32     5      377.56\n",
      "1           2    10      439.68       677.76     5      345.52\n",
      "2           3    10      477.38       736.37     5      428.83\n",
      "3           4    10      370.21       614.22     5      349.21\n",
      "4           5    10      460.64       634.78     5      397.01\n",
      "5           6    10      475.32       685.18     5      368.67\n",
      "6           7    10      409.38       614.44     5      332.20\n",
      "7           8    10      424.39       716.06     5      336.35\n",
      "8           9    10      526.75       786.46     5      370.54\n",
      "9          10    10      452.80       667.27     5      358.86\n",
      "\n",
      "Averages (MSE ↓) across outer folds:\n",
      "Linear regression: 449.94\n",
      "Baseline:          686.49\n",
      "Neural Network:    366.48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cross_validation(X, y, outer_folds=10, inner_folds=10, random_state=42):\n",
    "\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    lambda_grid = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "    rows = []\n",
    "    CV_outer = KFold(n_splits=outer_folds, shuffle=True, random_state=random_state)\n",
    "    outer_test_mse = []\n",
    "    CV_inner = KFold(n_splits=inner_folds, shuffle=True, random_state=random_state)\n",
    "    inner_mse = []\n",
    "\n",
    "\n",
    "    for outer_fold_idx, (outer_train_idx, outer_test_idx) in enumerate(CV_outer.split(X), start=1):\n",
    "        X_train_outer, X_test_outer = X.iloc[outer_train_idx], X.iloc[outer_test_idx]\n",
    "        y_train_outer, y_test_outer = y.iloc[outer_train_idx], y.iloc[outer_test_idx]\n",
    "\n",
    "        #standerdize data \n",
    "        scaler = StandardScaler()\n",
    "        X_train_outer = pd.DataFrame(scaler.fit_transform(X_train_outer), columns=X_train_outer.columns)\n",
    "        X_test_outer = pd.DataFrame(scaler.transform(X_test_outer), columns=X_test_outer.columns)\n",
    "    \n",
    "\n",
    "        lambda_errors = {}\n",
    "        for lam in lambda_grid:\n",
    "            inner_mses = []\n",
    "            for inner_train_idx, inner_test_idx in CV_inner.split(X_train_outer):\n",
    "                X_train_inner = X_train_outer.iloc[inner_train_idx]\n",
    "                X_test_inner = X_train_outer.iloc[inner_test_idx]\n",
    "                y_train_inner = y_train_outer.iloc[inner_train_idx]\n",
    "                y_test_inner = y_train_outer.iloc[inner_test_idx]\n",
    "\n",
    "                # LINEAR REGRESSION MODEL\n",
    "                model = Ridge(alpha=lam)\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                y_pred_inner = model.predict(X_test_inner)\n",
    "                inner_mse.append(mean_squared_error(y_test_inner, y_pred_inner))\n",
    "                E_test_lin = mean_squared_error(y_test_outer,  model.predict(X_test_outer))\n",
    "                inner_mses.append(mean_squared_error(y_test_inner, y_pred_inner))\n",
    "                \n",
    "            lambda_errors[lam] = np.mean(inner_mses)\n",
    "\n",
    "        best_lambda = min(lambda_errors, key=lambda_errors.get)\n",
    "\n",
    "        # TRAIN FINAL MODEL ON FULL TRAINING SET WITH BEST LAMBDA\n",
    "        model = Ridge(alpha=best_lambda)\n",
    "        model.fit(X_train_outer, y_train_outer)\n",
    "        E_test_lin = mean_squared_error(y_test_outer, model.predict(X_test_outer))\n",
    "        outer_test_mse.append(E_test_lin)\n",
    "\n",
    "        # BASELINE MODEL\n",
    "        baseline_pred = float(np.mean(y_train_outer))       \n",
    "        E_test_base = mean_squared_error(y_test_outer, np.full(len(y_test_outer), baseline_pred))\n",
    "\n",
    "        # ANN MODEL \n",
    "        lr = 0.01\n",
    "        n_epochs = 1000 \n",
    "        input_dim  = X_train_outer.shape[1]  # Number of features\n",
    "        hidden_dim = 2\n",
    "        output_dim = 1\n",
    "        hyperparameters_to_tune = [1, 2, 3, 4, 5]\n",
    "        best_h, best_val = None, np.inf\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        def get_model(input_dim, hidden_dim, output_dim):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=True),     # Input layer\n",
    "                torch.nn.Tanh(),                                                                # Activation function\n",
    "                torch.nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True),    # Output layer\n",
    "            )\n",
    "\n",
    "        # Loop over the hyperparameter settings        \n",
    "        for hidden_dim in hyperparameters_to_tune:\n",
    "            fold_vals = []\n",
    "            for inner_train_idx, inner_test_idx in CV_inner.split(X_train_outer):\n",
    "                X_train_inner = X_train_outer.iloc[inner_train_idx]\n",
    "                X_test_inner = X_train_outer.iloc[inner_test_idx]\n",
    "                y_train_inner = y_train_outer.iloc[inner_train_idx]\n",
    "                y_test_inner = y_train_outer.iloc[inner_test_idx]\n",
    "\n",
    "                X_train_inner_tensor = torch.tensor(X_train_inner.values, dtype=torch.float32)\n",
    "                y_train_inner_tensor = torch.tensor(y_train_inner.values, dtype=torch.float32).view(-1, 1)\n",
    "                X_test_inner_tensor  = torch.tensor(X_test_inner.values, dtype=torch.float32)\n",
    "                y_test_inner_tensor  = torch.tensor(y_test_inner.values, dtype=torch.float32).view(-1, 1)\n",
    "            \n",
    "                model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "                optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "            \n",
    "                for _ in range(n_epochs):\n",
    "                    model.train()\n",
    "                    outputs = model(X_train_inner_tensor)\n",
    "                    loss = criterion(outputs, y_train_inner_tensor)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    fold_vals.append(criterion(model(X_test_inner_tensor), y_test_inner_tensor).item())\n",
    "\n",
    "            val_loss = np.mean(fold_vals)\n",
    "            if val_loss < best_val:\n",
    "                best_val = val_loss\n",
    "                best_h = hidden_dim\n",
    "\n",
    "        # TRAIN FINAL MODEL ON FULL TRAINING SET WITH BEST HYPERPARAMETER\n",
    "        X_train_outer_tensor = torch.tensor(X_train_outer.values.astype(np.float32))\n",
    "        y_train_outer_tensor = torch.tensor(y_train_outer.values.astype(np.float32).reshape(-1, 1))\n",
    "        X_test_outer_tensor  = torch.tensor(X_test_outer.values.astype(np.float32))\n",
    "\n",
    "        model = get_model(input_dim, best_h, output_dim)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        for _ in range(n_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_train_outer_tensor), y_train_outer_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            E_test_ann = mean_squared_error(y_test_outer.values, model(X_test_outer_tensor).detach().numpy().ravel())\n",
    "\n",
    "        rows.append({\"Outer fold\": outer_fold_idx,  \"λ*_i\": best_lambda, \"E_test^lin\": E_test_lin, \"E_test^base\": E_test_base,  \"h*_i\": best_h, \"E_test^ann\": E_test_ann})\n",
    "\n",
    "    results = pd.DataFrame(rows).sort_values(\"Outer fold\").reset_index(drop=True)\n",
    "    print(results.round({\"E_test^lin\":2, \"E_test^base\":2, \"E_test^ann\":2}))\n",
    "    print(\"\\nAverages (MSE ↓) across outer folds:\")\n",
    "    print(f\"Linear regression: {results['E_test^lin'].mean():.2f}\")\n",
    "    print(f\"Baseline:          {results['E_test^base'].mean():.2f}\")\n",
    "    print(f\"Neural Network:    {results['E_test^ann'].mean():.2f}\")\n",
    "\n",
    "    return results\n",
    "results  = cross_validation(X, y, outer_folds=10, inner_folds=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd9023",
   "metadata": {},
   "source": [
    "*STATS EVALUATION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "911d3e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear vs Baseline:\n",
      "mean diff = -236.551\n",
      "t = -21.103, p = 5.651823060668685e-09\n",
      "95% CI = [-261.907, -211.196]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# example: linear vs baseline\n",
    "d_lin_base = results[\"E_test^lin\"] - results[\"E_test^base\"]\n",
    "t_stat, p_val = stats.ttest_rel(results[\"E_test^lin\"], results[\"E_test^base\"])\n",
    "\n",
    "mean_diff = d_lin_base.mean()\n",
    "std_diff  = d_lin_base.std(ddof=1)\n",
    "n = len(d_lin_base)\n",
    "ci_low  = mean_diff - 2.262 * std_diff / np.sqrt(n)\n",
    "ci_high = mean_diff + 2.262 * std_diff / np.sqrt(n)\n",
    "\n",
    "print(f\"Linear vs Baseline:\")\n",
    "print(f\"mean diff = {mean_diff:.3f}\")\n",
    "print(f\"t = {t_stat:.3f}, p = {p_val}\")\n",
    "print(f\"95% CI = [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6326182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear vs ANN:\n",
      "mean diff = -83.461\n",
      "t = -7.341, p = 4.3670510870121584e-05\n",
      "95% CI = [-109.176, -57.745]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# example: linear vs ann\n",
    "d_lin_base = results[\"E_test^ann\"] - results[\"E_test^lin\"]\n",
    "t_stat, p_val = stats.ttest_rel(results[\"E_test^ann\"], results[\"E_test^lin\"])\n",
    "\n",
    "mean_diff = d_lin_base.mean()\n",
    "std_diff  = d_lin_base.std(ddof=1)\n",
    "n = len(d_lin_base)\n",
    "ci_low  = mean_diff - 2.262 * std_diff / np.sqrt(n)\n",
    "ci_high = mean_diff + 2.262 * std_diff / np.sqrt(n)\n",
    "\n",
    "print(f\"Linear vs ANN:\")\n",
    "print(f\"mean diff = {mean_diff:.3f}\")\n",
    "print(f\"t = {t_stat:.3f}, p = {p_val}\")\n",
    "print(f\"95% CI = [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "179c9884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear vs ANN:\n",
      "mean diff = 83.461\n",
      "t = -18.961, p = 1.4529217140698072e-08\n",
      "95% CI = [57.745, 109.176]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# example: ann vs baseline\n",
    "d_ann_base = results[\"E_test^ann\"] - results[\"E_test^base\"]\n",
    "t_stat, p_val = stats.ttest_rel(results[\"E_test^ann\"], results[\"E_test^base\"])\n",
    "\n",
    "mean_diff = d_lin_base.mean()\n",
    "std_diff  = d_lin_base.std(ddof=1)\n",
    "n = len(d_lin_base)\n",
    "ci_low  = mean_diff - 2.262 * std_diff / np.sqrt(n)\n",
    "ci_high = mean_diff + 2.262 * std_diff / np.sqrt(n)\n",
    "\n",
    "print(f\"Linear vs ANN:\")\n",
    "print(f\"mean diff = {mean_diff:.3f}\")\n",
    "print(f\"t = {t_stat:.3f}, p = {p_val}\")\n",
    "print(f\"95% CI = [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
