{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb7f4cc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['CAEC', 'CALC'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     33\u001b[39m freq_cols = [\u001b[33m'\u001b[39m\u001b[33mCAEC\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mCALC\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     36\u001b[39m X = pd.get_dummies(X, columns=[\u001b[33m'\u001b[39m\u001b[33mMTRANS\u001b[39m\u001b[33m'\u001b[39m], drop_first=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m X = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCAEC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCALC\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m obesity_order = [\u001b[33m\"\u001b[39m\u001b[33mInsufficient_Weight\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mNormal_Weight\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mOverweight_Level_I\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mOverweight_Level_II\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mObesity_Type_I\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mObesity_Type_II\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mObesity_Type_III\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     41\u001b[39m X[\u001b[33m\"\u001b[39m\u001b[33mNObeyesdad\u001b[39m\u001b[33m\"\u001b[39m] = X[\u001b[33m\"\u001b[39m\u001b[33mNObeyesdad\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: obesity_order.index(x) + \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:170\u001b[39m, in \u001b[36mget_dummies\u001b[39m\u001b[34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[39m\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput must be a list-like for parameter `columns`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     data_to_encode = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_len\u001b[39m(item, name: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4112\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4115\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6259\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6260\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['CAEC', 'CALC'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.gridspec as gridspec\n",
    "from sklearn.model_selection import train_test_split, KFold, LeaveOneOut\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Ler o dataset\n",
    "df = pd.read_csv(\"C:\\\\Users\\\\carlo\\\\OneDrive - Danmarks Tekniske Universitet\\\\1st Year 1st Semester\\\\02452 Machine Learning\\\\project1-ml\\\\data\\\\ObesityDataSet_Clean.csv\")\n",
    "\n",
    "categorical_cols = ['Gender', 'SMOKE', 'family_history_with_overweight', 'CAEC', 'MTRANS', 'NObeyesdad']\n",
    "\n",
    "binary_cols = ['family_history_with_overweight', 'FAVC', 'SMOKE', 'SCC']\n",
    "for col in binary_cols:\n",
    "        X[col] = X[col].map({'yes': 1, 'no': 0})\n",
    "\n",
    "freq_cols = ['CAEC', 'CALC']\n",
    "\n",
    "\n",
    "X = pd.get_dummies(X, columns=['MTRANS'], drop_first=True)\n",
    "X = pd.get_dummies(X, columns=['CAEC', 'CALC'], drop_first=True)\n",
    "\n",
    "\n",
    "obesity_order = [\"Insufficient_Weight\",\"Normal_Weight\",\"Overweight_Level_I\",\"Overweight_Level_II\",\"Obesity_Type_I\",\"Obesity_Type_II\",\"Obesity_Type_III\"]\n",
    "X[\"NObeyesdad\"] = X[\"NObeyesdad\"].apply(lambda x: obesity_order.index(x) + 1)\n",
    "\n",
    "#feature_cols = [\"Age\", \"Height\", \"FCVC\", \"NCP\", \"CH2O\", \"FAF\", \"TUE\"]\n",
    "\n",
    "# define target and features (just to extract y)\n",
    "y = X[\"Weight\"]\n",
    "X = X.drop(columns=[\"Weight\"])\n",
    "\n",
    "# baseline ignores X, but we still need to split the same way\n",
    "print(X.dtypes)\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29904799",
   "metadata": {},
   "source": [
    "*FIND BEST SET OF H (ANN)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df41278e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not convert ['nonononoyesyesnoyesyesnoyesyesyesnoyesyesyesnononoyesyesyesnoyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesnonononoyesnoyesnoyesnoyesyesyesnononoyesnononononoyesnoyesyesyesyesyesnoyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesnoyesyesyesnonoyesyesyesnonoyesyesyesnoyesyesnononononononoyesyesnonoyesyesyesyesyesyesyesyesnonoyesyesnoyesyesyesyesnoyesnoyesyesyesyesnoyesnonoyesyesyesnoyesnoyesyesnoyesnoyesyesyesyesyesyesyesnoyesyesyesyesnononononoyesyesyesyesyesyesnoyesyesyesyesyesnoyesyesyesyesyesyesyesyesnonoyesnoyesnoyesyesyesnoyesyesnoyesnoyesyesyesnonoyesyesyesyesyesyesyesyesnoyesnonoyesyesnoyesyesnoyesyesyesnoyesnonoyesyesyesyesyesyesnoyesyesyesyesyesyesnoyesnonoyesnoyesnoyesyesnoyesyesnoyesnoyesnoyesyesnononoyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesnonoyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesnononoyesyesyesyesyesnoyesyesyesyesyesyesnononoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesnononoyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesnonononoyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnononoyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnononoyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesnonononoyesyesyesyesyesnonoyesyesyesyesyesyesyesnoyesnonoyesyesyesnoyesyesyesyesyesyesyesyesyesnoyesnoyesyesyesyesyesnonoyesyesyesyesyesyesyesyesnoyesyesnonoyesnonoyesyesnoyesyesyesyesyesnoyesyesyesyesyesyesnoyesyesnoyesnonoyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesyesnononoyesyesyesyesyesnonoyesnoyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes'\n 'nononononoyesnononononononononoyesnonononoyesnononononononononononononononononoyesnoyesyesnonoyesnononoyesnonoyesnoyesnononononononononoyesnonononononononononononononononononoyesnonononoyesnononoyesnononononononononononononoyesnononononononoyesnonononononononononononoyesnonononononoyesnonononononononononononononononoyesnonoyesnonoyesnononoyesnonoyesnononoyesnoyesnononononononononononononononononononononononononononononononononononononononononononononoyesnonoyesyesnonononononononononononononononononononononononoyesnononononoyesnononononononononononononononononononoyesyesnonononoyesyesnononononoyesnonononononononononononononononononononononononononononoyesnonoyesnononononoyesnononoyesnonononononononononononononoyesnononononononononononononononononononononononononononononononononononononononononononononononononononoyesyesnonononononononoyesnononononononononononononononononoyesnononononononononoyesnononoyesnononononononononononononononononononononononoyesnonononononononononononononononononononononoyesyesyesnononononononononononononononononononononononononononononononononononoyesyesnonononoyesnononononoyesnonononononononononononononononononononononononononononononononononononononononoyesyesnononononononoyesyesnoyesnonononononononononononononononononononononononononononononononononononononononononononononoyesyesnononononoyesyesyesnononononononoyesnononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononono'\n 'noFrequentlyFrequentlySometimesFrequentlySometimesSometimesFrequentlySometimesnonoSometimesSometimesnoAlwaysSometimesFrequentlySometimesSometimesSometimesnoSometimesSometimesnonoFrequentlynoSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesnoFrequentlyFrequentlySometimesSometimesnoSometimesFrequentlynonoSometimesSometimesnoSometimesnononoFrequentlynonoFrequentlySometimesSometimesSometimesFrequentlySometimesnoSometimesSometimesSometimesSometimesSometimesnonoSometimesnoSometimesFrequentlynonoSometimesnoFrequentlynoFrequentlySometimesSometimesSometimesFrequentlynonoFrequentlySometimesSometimesnononoSometimesSometimesFrequentlySometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesnoSometimesnoSometimesnoSometimesnoFrequentlySometimesnoSometimesSometimesSometimesSometimesFrequentlyFrequentlynonoSometimesSometimesSometimesSometimesSometimesFrequentlySometimesSometimesSometimesnonononoSometimesnoSometimesnonoSometimesnonoSometimesSometimesnononononononononoSometimesSometimesSometimesnoSometimesnonoSometimesnonoSometimesnoSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesnonoSometimesnonononoSometimesnoSometimesSometimesSometimesSometimesSometimesnonoSometimesnoSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoFrequentlynonoSometimesFrequentlynoSometimesnoSometimesSometimesnonoSometimesSometimesSometimesnoSometimesnonoFrequentlySometimesSometimesSometimesSometimesSometimesSometimesnoSometimesnoSometimesnoSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesnonononoSometimesFrequentlynoSometimesnoSometimesnoSometimesSometimesSometimesSometimesnoSometimesSometimesnoFrequentlySometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesnoSometimesnoSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesnonoSometimesSometimesSometimesnonoSometimesnoSometimesnonononoSometimesSometimesnonoSometimesSometimesnoSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnononononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesnononoSometimesSometimesSometimesSometimesnonoSometimesSometimesnonoSometimesSometimesSometimesSometimesSometimesnononoSometimesSometimesSometimesSometimesSometimesnononononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesnoSometimesSometimesnoSometimesSometimesnoSometimesnoSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesnonononononononoSometimesnonoSometimesSometimesSometimesSometimesnonoSometimesnoSometimesnoSometimesSometimesSometimesSometimesnononoSometimesnoSometimesSometimesSometimesnonononoSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesnononoSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesFrequentlySometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesnoSometimesnonoSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesnonoSometimesSometimesFrequentlyFrequentlynonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesFrequentlySometimesnonoFrequentlySometimesSometimesSometimesSometimesSometimesnoSometimesnononoSometimesnoSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesnoFrequentlynononononononoSometimesnonoSometimesSometimesSometimesnonoSometimesSometimesnoSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnononoSometimesnononoSometimesnononoSometimesnoSometimesSometimesnonoFrequentlySometimesSometimesSometimesFrequentlynoFrequentlynononoSometimesnoSometimesSometimesSometimesSometimesnoSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesnoSometimesnoSometimesnononoSometimesnoSometimesnonoSometimesnonoSometimesSometimesFrequentlySometimesSometimesSometimesSometimesSometimesSometimesSometimesnoFrequentlynonononononononoSometimesnonononoSometimesnoSometimesSometimesnoSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesnonoSometimesSometimesSometimesSometimesSometimesnoSometimesnonononoSometimesSometimesnoSometimesSometimesnoSometimesnoFrequentlynonoSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesFrequentlynononoSometimesnonononononoSometimesSometimesnononoSometimesSometimesnonoSometimesSometimesnoSometimesnoSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesnoSometimesSometimesSometimesnonononononoSometimesSometimesSometimesSometimesSometimesFrequentlySometimesSometimesnonoSometimesSometimesSometimesnonononononoSometimesnononononoSometimesnonoSometimesSometimesnonononoSometimesSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesnononononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesnoSometimesSometimesnonononoSometimesSometimesnonononononononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesnonoSometimesSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesnoSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimes'] to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     34\u001b[39m y_train, y_val = y[train_index], y[val_index]\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Normalize data here\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m mean, std = \u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m, X_train.std(axis=\u001b[32m0\u001b[39m)\n\u001b[32m     38\u001b[39m X_train = (X_train - mean) / std\n\u001b[32m     39\u001b[39m X_val = (X_val - mean) / std\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\frame.py:11700\u001b[39m, in \u001b[36mDataFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11692\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m2\u001b[39m))\n\u001b[32m  11693\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  11694\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11698\u001b[39m     **kwargs,\n\u001b[32m  11699\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m11700\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11701\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[32m  11702\u001b[39m         result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\generic.py:12439\u001b[39m, in \u001b[36mNDFrame.mean\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12432\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmean\u001b[39m(\n\u001b[32m  12433\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12434\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12437\u001b[39m     **kwargs,\n\u001b[32m  12438\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12439\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12440\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  12441\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\generic.py:12396\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12392\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12394\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12398\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\frame.py:11569\u001b[39m, in \u001b[36mDataFrame._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m  11565\u001b[39m     df = df.T\n\u001b[32m  11567\u001b[39m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[32m  11568\u001b[39m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11569\u001b[39m res = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11570\u001b[39m out = df._constructor_from_mgr(res, axes=res.axes).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m  11571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out.dtype != \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1500\u001b[39m, in \u001b[36mBlockManager.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m   1498\u001b[39m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   1499\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1500\u001b[39m     nbs = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1501\u001b[39m     res_blocks.extend(nbs)\n\u001b[32m   1503\u001b[39m index = Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:406\u001b[39m, in \u001b[36mBlock.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    409\u001b[39m         res_values = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\frame.py:11488\u001b[39m, in \u001b[36mDataFrame._reduce.<locals>.blk_func\u001b[39m\u001b[34m(values, axis)\u001b[39m\n\u001b[32m  11486\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array([result])\n\u001b[32m  11487\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m> \u001b[39m\u001b[32m11488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\nanops.py:404\u001b[39m, in \u001b[36m_datetimelike_compat.<locals>.new_func\u001b[39m\u001b[34m(values, axis, skipna, mask, **kwargs)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    402\u001b[39m     mask = isna(values)\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[32m    407\u001b[39m     result = _wrap_results(result, orig_values.dtype, fill_value=iNaT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\nanops.py:720\u001b[39m, in \u001b[36mnanmean\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m    718\u001b[39m count = _get_counts(values.shape, mask, axis, dtype=dtype_count)\n\u001b[32m    719\u001b[39m the_sum = values.sum(axis, dtype=dtype_sum)\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m the_sum = \u001b[43m_ensure_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthe_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[33m\"\u001b[39m\u001b[33mndim\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    723\u001b[39m     count = cast(np.ndarray, count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\carlo\\miniforge3\\envs\\dtu02452\\Lib\\site-packages\\pandas\\core\\nanops.py:1686\u001b[39m, in \u001b[36m_ensure_numeric\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m   1683\u001b[39m inferred = lib.infer_dtype(x)\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inferred \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1686\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to numeric\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1688\u001b[39m     x = x.astype(np.complex128)\n",
      "\u001b[31mTypeError\u001b[39m: Could not convert ['nonononoyesyesnoyesyesnoyesyesyesnoyesyesyesnononoyesyesyesnoyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesnonononoyesnoyesnoyesnoyesyesyesnononoyesnononononoyesnoyesyesyesyesyesnoyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesnoyesyesyesnonoyesyesyesnonoyesyesyesnoyesyesnononononononoyesyesnonoyesyesyesyesyesyesyesyesnonoyesyesnoyesyesyesyesnoyesnoyesyesyesyesnoyesnonoyesyesyesnoyesnoyesyesnoyesnoyesyesyesyesyesyesyesnoyesyesyesyesnononononoyesyesyesyesyesyesnoyesyesyesyesyesnoyesyesyesyesyesyesyesyesnonoyesnoyesnoyesyesyesnoyesyesnoyesnoyesyesyesnonoyesyesyesyesyesyesyesyesnoyesnonoyesyesnoyesyesnoyesyesyesnoyesnonoyesyesyesyesyesyesnoyesyesyesyesyesyesnoyesnonoyesnoyesnoyesyesnoyesyesnoyesnoyesnoyesyesnononoyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesnonoyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesnononoyesyesyesyesyesnoyesyesyesyesyesyesnononoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesnononoyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesnonononoyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnononoyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnononoyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesnonononoyesyesyesyesyesnonoyesyesyesyesyesyesyesnoyesnonoyesyesyesnoyesyesyesyesyesyesyesyesyesnoyesnoyesyesyesyesyesnonoyesyesyesyesyesyesyesyesnoyesyesnonoyesnonoyesyesnoyesyesyesyesyesnoyesyesyesyesyesyesnoyesyesnoyesnonoyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesnonoyesyesyesyesyesyesyesnononoyesyesyesyesyesnonoyesnoyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesnoyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyesyes'\n 'nononononoyesnononononononononoyesnonononoyesnononononononononononononononononoyesnoyesyesnonoyesnononoyesnonoyesnoyesnononononononononoyesnonononononononononononononononononoyesnonononoyesnononoyesnononononononononononononoyesnononononononoyesnonononononononononononoyesnonononononoyesnonononononononononononononononoyesnonoyesnonoyesnononoyesnonoyesnononoyesnoyesnononononononononononononononononononononononononononononononononononononononononononononoyesnonoyesyesnonononononononononononononononononononononononoyesnononononoyesnononononononononononononononononononoyesyesnonononoyesyesnononononoyesnonononononononononononononononononononononononononononoyesnonoyesnononononoyesnononoyesnonononononononononononononoyesnononononononononononononononononononononononononononononononononononononononononononononononononononoyesyesnonononononononoyesnononononononononononononononononoyesnononononononononoyesnononoyesnononononononononononononononononononononononoyesnonononononononononononononononononononononoyesyesyesnononononononononononononononononononononononononononononononononononoyesyesnonononoyesnononononoyesnonononononononononononononononononononononononononononononononononononononononoyesyesnononononononoyesyesnoyesnonononononononononononononononononononononononononononononononononononononononononononononoyesyesnononononoyesyesyesnononononononoyesnononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononononono'\n 'noFrequentlyFrequentlySometimesFrequentlySometimesSometimesFrequentlySometimesnonoSometimesSometimesnoAlwaysSometimesFrequentlySometimesSometimesSometimesnoSometimesSometimesnonoFrequentlynoSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesnoFrequentlyFrequentlySometimesSometimesnoSometimesFrequentlynonoSometimesSometimesnoSometimesnononoFrequentlynonoFrequentlySometimesSometimesSometimesFrequentlySometimesnoSometimesSometimesSometimesSometimesSometimesnonoSometimesnoSometimesFrequentlynonoSometimesnoFrequentlynoFrequentlySometimesSometimesSometimesFrequentlynonoFrequentlySometimesSometimesnononoSometimesSometimesFrequentlySometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesnoSometimesnoSometimesnoSometimesnoFrequentlySometimesnoSometimesSometimesSometimesSometimesFrequentlyFrequentlynonoSometimesSometimesSometimesSometimesSometimesFrequentlySometimesSometimesSometimesnonononoSometimesnoSometimesnonoSometimesnonoSometimesSometimesnononononononononoSometimesSometimesSometimesnoSometimesnonoSometimesnonoSometimesnoSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesnonoSometimesnonononoSometimesnoSometimesSometimesSometimesSometimesSometimesnonoSometimesnoSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoFrequentlynonoSometimesFrequentlynoSometimesnoSometimesSometimesnonoSometimesSometimesSometimesnoSometimesnonoFrequentlySometimesSometimesSometimesSometimesSometimesSometimesnoSometimesnoSometimesnoSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesnonononoSometimesFrequentlynoSometimesnoSometimesnoSometimesSometimesSometimesSometimesnoSometimesSometimesnoFrequentlySometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesnoSometimesnoSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesnonoSometimesSometimesSometimesnonoSometimesnoSometimesnonononoSometimesSometimesnonoSometimesSometimesnoSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnononononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesnononoSometimesSometimesSometimesSometimesnonoSometimesSometimesnonoSometimesSometimesSometimesSometimesSometimesnononoSometimesSometimesSometimesSometimesSometimesnononononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesnoSometimesSometimesnoSometimesSometimesnoSometimesnoSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesnonononononononoSometimesnonoSometimesSometimesSometimesSometimesnonoSometimesnoSometimesnoSometimesSometimesSometimesSometimesnononoSometimesnoSometimesSometimesSometimesnonononoSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesnononoSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesFrequentlySometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesnoSometimesnonoSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesnonoSometimesSometimesFrequentlyFrequentlynonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesFrequentlySometimesnonoFrequentlySometimesSometimesSometimesSometimesSometimesnoSometimesnononoSometimesnoSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesnoFrequentlynononononononoSometimesnonoSometimesSometimesSometimesnonoSometimesSometimesnoSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnononoSometimesnononoSometimesnononoSometimesnoSometimesSometimesnonoFrequentlySometimesSometimesSometimesFrequentlynoFrequentlynononoSometimesnoSometimesSometimesSometimesSometimesnoSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesnoSometimesnoSometimesnononoSometimesnoSometimesnonoSometimesnonoSometimesSometimesFrequentlySometimesSometimesSometimesSometimesSometimesSometimesSometimesnoFrequentlynonononononononoSometimesnonononoSometimesnoSometimesSometimesnoSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesnonoSometimesSometimesSometimesSometimesSometimesnoSometimesnonononoSometimesSometimesnoSometimesSometimesnoSometimesnoFrequentlynonoSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesFrequentlynononoSometimesnonononononoSometimesSometimesnononoSometimesSometimesnonoSometimesSometimesnoSometimesnoSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesnoSometimesSometimesSometimesnonononononoSometimesSometimesSometimesSometimesSometimesFrequentlySometimesSometimesnonoSometimesSometimesSometimesnonononononoSometimesnononononoSometimesnonoSometimesSometimesnonononoSometimesSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesnononononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesnoSometimesSometimesnonononoSometimesSometimesnonononononononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesnonoSometimesSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesnoSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonoSometimesSometimesSometimesSometimesnononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesnoSometimesSometimesSometimesSometimesSometimesSometimesSometimesnonononoSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimesSometimes'] to numeric"
     ]
    }
   ],
   "source": [
    "def get_model(input_dim, hidden_dim, output_dim):\n",
    "    ### BEGIN SOLUTION\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=True),     # Input layer\n",
    "        torch.nn.Tanh(),                                                                # Activation function\n",
    "        torch.nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True),    # Output layer\n",
    "    )\n",
    "\n",
    "mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_val = (X_val - mean) / std\n",
    "\n",
    "\n",
    "# K-fold crossvalidation\n",
    "K = 3\n",
    "CV = KFold(K, shuffle=True, random_state=0)\n",
    "\n",
    "# Define hyperparameters\n",
    "lr = 1e-3\n",
    "n_epochs = 1000\n",
    "\n",
    "# Seed for reproducibility\n",
    "seed = 0\n",
    "\n",
    "# Hyperparameter tuning loop with K-fold crossvalidation\n",
    "hyperparameters_to_tune = [1, 2, 3, 10, 15, 50]\n",
    "\n",
    "results = {}\n",
    "for k, (train_index, val_index) in enumerate(CV.split(X, y)):\n",
    "    print(f'Fold {k+1}/{K}')\n",
    "    \n",
    "    # Get the training and test data for this fold\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    # Normalize data here\n",
    "    mean, std = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "    X_train = (X_train - mean) / std\n",
    "    X_val = (X_val - mean) / std\n",
    "\n",
    "    # Convert to torch tensors\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "    X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    input_dim  = X_train_tensor.shape[1]  # Number of features\n",
    "    hidden_dim = 2\n",
    "    output_dim = 1\n",
    "\n",
    "\n",
    "    # Set up a dictionary to store the results for each hyperparameter setting\n",
    "    results_inner = {hidden_dim: {'train': [], 'val': []} for hidden_dim in hyperparameters_to_tune}\n",
    "\n",
    "    # Loop over the hyperparameter settings        \n",
    "    for hidden_dim in hyperparameters_to_tune:\n",
    "        # Define a model instance with a specific number of hidden units\n",
    "        model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "\n",
    "        # Define loss criterion\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        # Define the optimizer as the Adam optimizer (not needed to know the details)\n",
    "        optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Implement the training loop here\n",
    "            # Set the model to training mode\n",
    "            model.train()\n",
    "\n",
    "            # Make a forward pass through the model to compute the outputs\n",
    "            outputs = model(X_train_tensor)\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "            # Make sure that the gradients are zero before you use backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            # Do a backward pass to compute the gradients wrt. model parameters using backpropagation.\n",
    "            loss.backward()\n",
    "            # Update the model parameters by making the optimizer take a gradient descent step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Store the training loss for this epoch in the dictionary\n",
    "            results_inner[hidden_dim]['train'].append(loss.item())\n",
    "\n",
    "        # Compute the final test loss on the test set\n",
    "        with torch.no_grad(): # No need to compute gradients for the validation set\n",
    "            model.eval()\n",
    "            val_outputs = model(X_val_tensor)\n",
    "            val_loss = criterion(val_outputs, y_val_tensor)\n",
    "            results_inner[hidden_dim]['val'].append(val_loss.item())\n",
    "            print(f'  Hidden units: {hidden_dim}, Validation set MSE: {val_loss.item():.4f}')\n",
    "\n",
    "    # Store the results for this fold\n",
    "    results[k] = results_inner\n",
    "\n",
    "\n",
    "# Plot the loss curves for each fold and hyperparameter setting\n",
    "fig, axs = plt.subplots(1, K, figsize=(12, 4), sharey=True, sharex=True)\n",
    "# Plot the training loss for each fold and hyperparameter setting\n",
    "for fold in range(K):\n",
    "    for hidden_dim in hyperparameters_to_tune:\n",
    "        # Plot the training loss for this hyperparameter setting\n",
    "        axs[fold].plot(results[fold][hidden_dim]['train'], label=f'hidden_dim={hidden_dim}')\n",
    "\n",
    "    # Set the title and labels for each subplot\n",
    "    axs[fold].set_title('Fold {}'.format(fold+1))\n",
    "    axs[fold].set_xlabel('Epoch')\n",
    "    axs[fold].set_ylabel('MSE')\n",
    "\n",
    "# Set the overall title and show the legend\n",
    "plt.suptitle('Training loss for different hidden units')\n",
    "plt.tight_layout()\n",
    "axs[0].legend()\n",
    "plt.show()\n",
    "\n",
    "print(y.min(), y.max(), y.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e27cc80",
   "metadata": {},
   "source": [
    "*TWO LAYER CROSS VALIDATION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad02c922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Outer fold  *_i  E_test^lin  E_test^base  h*_i  E_test^ann\n",
      "0           1    10      462.83       732.32     5      347.48\n",
      "1           2    10      439.68       677.76     5      368.58\n",
      "2           3    10      477.38       736.37     5      413.90\n",
      "3           4    10      370.21       614.22     5      350.27\n",
      "4           5    10      460.64       634.78     5      386.83\n",
      "5           6    10      475.32       685.18     5      414.28\n",
      "6           7    10      409.38       614.44     5      334.87\n",
      "7           8    10      424.39       716.06     5      348.40\n",
      "8           9    10      526.75       786.46     5      382.86\n",
      "9          10    10      452.80       667.27     5      354.89\n",
      "\n",
      "Averages (MSE ) across outer folds:\n",
      "Linear regression: 449.94\n",
      "Baseline:          686.49\n",
      "Neural Network:    370.24\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def cross_validation(X, y, outer_folds=10, inner_folds=10, random_state=42):\n",
    "\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "\n",
    "    lambda_grid = [1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\n",
    "    rows = []\n",
    "    CV_outer = KFold(n_splits=outer_folds, shuffle=True, random_state=random_state)\n",
    "    outer_test_mse = []\n",
    "    CV_inner = KFold(n_splits=inner_folds, shuffle=True, random_state=random_state)\n",
    "    inner_mse = []\n",
    "\n",
    "\n",
    "    for outer_fold_idx, (outer_train_idx, outer_test_idx) in enumerate(CV_outer.split(X), start=1):\n",
    "        X_train_outer, X_test_outer = X.iloc[outer_train_idx], X.iloc[outer_test_idx]\n",
    "        y_train_outer, y_test_outer = y.iloc[outer_train_idx], y.iloc[outer_test_idx]\n",
    "\n",
    "        #standerdize data \n",
    "        mean, std = X_train_outer.mean(axis=0), X_train_outer.std(axis=0)\n",
    "        X_train_outer = (X_train_outer - mean) / std\n",
    "        X_test_outer = (X_test_outer - mean) / std\n",
    "\n",
    "        lambda_errors = {}\n",
    "        for lam in lambda_grid:\n",
    "            inner_mses = []\n",
    "            for inner_train_idx, inner_test_idx in CV_inner.split(X_train_outer):\n",
    "                X_train_inner = X_train_outer.iloc[inner_train_idx]\n",
    "                X_test_inner = X_train_outer.iloc[inner_test_idx]\n",
    "                y_train_inner = y_train_outer.iloc[inner_train_idx]\n",
    "                y_test_inner = y_train_outer.iloc[inner_test_idx]\n",
    "\n",
    "                # LINEAR REGRESSION MODEL\n",
    "                model = Ridge(alpha=lam)\n",
    "                model.fit(X_train_inner, y_train_inner)\n",
    "                y_pred_inner = model.predict(X_test_inner)\n",
    "                inner_mse.append(mean_squared_error(y_test_inner, y_pred_inner))\n",
    "                E_test_lin = mean_squared_error(y_test_outer,  model.predict(X_test_outer))\n",
    "                inner_mses.append(mean_squared_error(y_test_inner, y_pred_inner))\n",
    "                \n",
    "            lambda_errors[lam] = np.mean(inner_mses)\n",
    "\n",
    "        best_lambda = min(lambda_errors, key=lambda_errors.get)\n",
    "\n",
    "        # TRAIN FINAL MODEL ON FULL TRAINING SET WITH BEST LAMBDA\n",
    "        model = Ridge(alpha=best_lambda)\n",
    "        model.fit(X_train_outer, y_train_outer)\n",
    "        E_test_lin = mean_squared_error(y_test_outer, model.predict(X_test_outer))\n",
    "        outer_test_mse.append(E_test_lin)\n",
    "\n",
    "        # BASELINE MODEL\n",
    "        baseline_pred = float(np.mean(y_train_outer))       \n",
    "        E_test_base = mean_squared_error(y_test_outer, np.full(len(y_test_outer), baseline_pred))\n",
    "\n",
    "        # ANN MODEL \n",
    "        lr = 0.01\n",
    "        n_epochs = 1000 \n",
    "        input_dim  = X_train_outer.shape[1]  # Number of features\n",
    "        hidden_dim = 2\n",
    "        output_dim = 1\n",
    "        hyperparameters_to_tune = [1, 2, 3, 4, 5]\n",
    "        best_h, best_val = None, np.inf\n",
    "        criterion = torch.nn.MSELoss()\n",
    "\n",
    "        def get_model(input_dim, hidden_dim, output_dim):\n",
    "            return torch.nn.Sequential(\n",
    "                torch.nn.Linear(in_features=input_dim, out_features=hidden_dim, bias=True),     # Input layer\n",
    "                torch.nn.Tanh(),                                                                # Activation function\n",
    "                torch.nn.Linear(in_features=hidden_dim, out_features=output_dim, bias=True),    # Output layer\n",
    "            )\n",
    "\n",
    "        # Loop over the hyperparameter settings        \n",
    "        for hidden_dim in hyperparameters_to_tune:\n",
    "            fold_vals = []\n",
    "            for inner_train_idx, inner_test_idx in CV_inner.split(X_train_outer):\n",
    "                X_train_inner = X_train_outer.iloc[inner_train_idx]\n",
    "                X_test_inner = X_train_outer.iloc[inner_test_idx]\n",
    "                y_train_inner = y_train_outer.iloc[inner_train_idx]\n",
    "                y_test_inner = y_train_outer.iloc[inner_test_idx]\n",
    "\n",
    "                X_train_inner_tensor = torch.tensor(X_train_inner.values, dtype=torch.float32)\n",
    "                y_train_inner_tensor = torch.tensor(y_train_inner.values, dtype=torch.float32).view(-1, 1)\n",
    "                X_test_inner_tensor  = torch.tensor(X_test_inner.values, dtype=torch.float32)\n",
    "                y_test_inner_tensor  = torch.tensor(y_test_inner.values, dtype=torch.float32).view(-1, 1)\n",
    "            \n",
    "                model = get_model(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "                optimizer = torch.optim.SGD(params=model.parameters(), lr=lr)\n",
    "            \n",
    "                for _ in range(n_epochs):\n",
    "                    model.train()\n",
    "                    outputs = model(X_train_inner_tensor)\n",
    "                    loss = criterion(outputs, y_train_inner_tensor)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    fold_vals.append(criterion(model(X_test_inner_tensor), y_test_inner_tensor).item())\n",
    "\n",
    "            val_loss = np.mean(fold_vals)\n",
    "            if val_loss < best_val:\n",
    "                best_val = val_loss\n",
    "                best_h = hidden_dim\n",
    "\n",
    "        # TRAIN FINAL MODEL ON FULL TRAINING SET WITH BEST HYPERPARAMETER\n",
    "        X_train_outer_tensor = torch.tensor(X_train_outer.values.astype(np.float32))\n",
    "        y_train_outer_tensor = torch.tensor(y_train_outer.values.astype(np.float32).reshape(-1, 1))\n",
    "        X_test_outer_tensor  = torch.tensor(X_test_outer.values.astype(np.float32))\n",
    "\n",
    "        model = get_model(input_dim, best_h, output_dim)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "        for _ in range(n_epochs):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_train_outer_tensor), y_train_outer_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            E_test_ann = mean_squared_error(y_test_outer.values, model(X_test_outer_tensor).detach().numpy().ravel())\n",
    "\n",
    "        rows.append({\"Outer fold\": outer_fold_idx,  \"*_i\": best_lambda, \"E_test^lin\": E_test_lin, \"E_test^base\": E_test_base,  \"h*_i\": best_h, \"E_test^ann\": E_test_ann})\n",
    "\n",
    "    results = pd.DataFrame(rows).sort_values(\"Outer fold\").reset_index(drop=True)\n",
    "    print(results.round({\"E_test^lin\":2, \"E_test^base\":2, \"E_test^ann\":2}))\n",
    "    print(\"\\nAverages (MSE ) across outer folds:\")\n",
    "    print(f\"Linear regression: {results['E_test^lin'].mean():.2f}\")\n",
    "    print(f\"Baseline:          {results['E_test^base'].mean():.2f}\")\n",
    "    print(f\"Neural Network:    {results['E_test^ann'].mean():.2f}\")\n",
    "\n",
    "    return results\n",
    "results  = cross_validation(X, y, outer_folds=10, inner_folds=10, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd9023",
   "metadata": {},
   "source": [
    "*STATS EVALUATION*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "911d3e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear vs Baseline:\n",
      "mean diff = -236.551\n",
      "t = -21.103, p = 5.651823060668685e-09\n",
      "95% CI = [-261.907, -211.196]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# example: linear vs baseline\n",
    "d_lin_base = results[\"E_test^lin\"] - results[\"E_test^base\"]\n",
    "t_stat, p_val = stats.ttest_rel(results[\"E_test^lin\"], results[\"E_test^base\"])\n",
    "\n",
    "mean_diff = d_lin_base.mean()\n",
    "std_diff  = d_lin_base.std(ddof=1)\n",
    "n = len(d_lin_base)\n",
    "ci_low  = mean_diff - 2.262 * std_diff / np.sqrt(n)\n",
    "ci_high = mean_diff + 2.262 * std_diff / np.sqrt(n)\n",
    "\n",
    "print(f\"Linear vs Baseline:\")\n",
    "print(f\"mean diff = {mean_diff:.3f}\")\n",
    "print(f\"t = {t_stat:.3f}, p = {p_val}\")\n",
    "print(f\"95% CI = [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6326182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear vs ANN:\n",
      "mean diff = -83.461\n",
      "t = -7.341, p = 4.3670510870121584e-05\n",
      "95% CI = [-109.176, -57.745]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# example: linear vs ann\n",
    "d_lin_base = results[\"E_test^ann\"] - results[\"E_test^lin\"]\n",
    "t_stat, p_val = stats.ttest_rel(results[\"E_test^ann\"], results[\"E_test^lin\"])\n",
    "\n",
    "mean_diff = d_lin_base.mean()\n",
    "std_diff  = d_lin_base.std(ddof=1)\n",
    "n = len(d_lin_base)\n",
    "ci_low  = mean_diff - 2.262 * std_diff / np.sqrt(n)\n",
    "ci_high = mean_diff + 2.262 * std_diff / np.sqrt(n)\n",
    "\n",
    "print(f\"Linear vs ANN:\")\n",
    "print(f\"mean diff = {mean_diff:.3f}\")\n",
    "print(f\"t = {t_stat:.3f}, p = {p_val}\")\n",
    "print(f\"95% CI = [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "179c9884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear vs ANN:\n",
      "mean diff = 83.461\n",
      "t = -18.961, p = 1.4529217140698072e-08\n",
      "95% CI = [57.745, 109.176]\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# example: ann vs baseline\n",
    "d_ann_base = results[\"E_test^ann\"] - results[\"E_test^base\"]\n",
    "t_stat, p_val = stats.ttest_rel(results[\"E_test^ann\"], results[\"E_test^base\"])\n",
    "\n",
    "mean_diff = d_lin_base.mean()\n",
    "std_diff  = d_lin_base.std(ddof=1)\n",
    "n = len(d_lin_base)\n",
    "ci_low  = mean_diff - 2.262 * std_diff / np.sqrt(n)\n",
    "ci_high = mean_diff + 2.262 * std_diff / np.sqrt(n)\n",
    "\n",
    "print(f\"Linear vs ANN:\")\n",
    "print(f\"mean diff = {mean_diff:.3f}\")\n",
    "print(f\"t = {t_stat:.3f}, p = {p_val}\")\n",
    "print(f\"95% CI = [{ci_low:.3f}, {ci_high:.3f}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtu02452",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
